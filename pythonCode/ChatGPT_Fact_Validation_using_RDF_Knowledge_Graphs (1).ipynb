{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Import the Libraries**"
      ],
      "metadata": {
        "id": "O21ebBE-tzEE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Grcdvjvkydo5"
      },
      "outputs": [],
      "source": [
        "!pip install SPARQLWrapper\n",
        "!pip install nltk\n",
        "!pip install -U sentence-transformers\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Convert results to JSON format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Function for Returning the top-K similar triples**"
      ],
      "metadata": {
        "id": "jfnYXxnht-1b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P5NnxcYi3TGy"
      },
      "outputs": [],
      "source": [
        "def most_similar(sentences,fullURIs, similarity_matrix,matrix,k):\n",
        "    if matrix=='Cosine Similarity':\n",
        "        similar_ix=np.argsort(similarity_matrix[0])[::-1]\n",
        "    i=0\n",
        "    retValue=\"\"\n",
        "    max=0\n",
        "    #print(\"The most similar properties of \"+fullURIs[0])\n",
        "    for ix in similar_ix:\n",
        "        if ix==0:\n",
        "            continue\n",
        "        i=i+1\n",
        "        if i == k+1:\n",
        "            break\n",
        "        retValue=retValue+str(similarity_matrix[0][ix])+ '\\t'+fullURIs[ix]+\" Most Similar Triples\\n\"\n",
        "        if(i==1):\n",
        "            #retValue=fullURIs[ix]\n",
        "            max=similarity_matrix[0][ix]\n",
        "        #if(i==2 and \"dbpedia\" in fullURIs[ix] and similarity_matrix[0][ix]==max):\n",
        "            #retValue=fullURIs[ix]\n",
        "    return retValue\n",
        "        #print (documentsLabels[ix])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Importing the wikidata labels for the properties**"
      ],
      "metadata": {
        "id": "0UGm1_p8uFhT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLloY-YAhgx7"
      },
      "outputs": [],
      "source": [
        "f = open(\"wikidata.txt\", \"r\")\n",
        "wkdProps={}\n",
        "for x in f:\n",
        "  prop=x.split(\",\")[0]\n",
        "  label=x.split(\",\")[1].replace(\"\\n\",\"\")\n",
        "  wkdProps[prop]=label\n",
        "print(wkdProps)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Functions for finding the most similar triples by using LODsyndesis**"
      ],
      "metadata": {
        "id": "EYdgTvvNuMbr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b6qZ5QMvSId_"
      },
      "outputs": [],
      "source": [
        "# A GET request to the API\n",
        "import re\n",
        "\n",
        "def getBestPredicate(entity,property,fullProperty):\n",
        "    url = \"https://demos.isl.ics.forth.gr/lodsyndesis/rest-api/allFacts?uri=\"+entity\n",
        "    sentences = [re.sub( '(?<!^)(?=[A-Z])', ' ',property).lower()]\n",
        "    response =requests.get(url, \n",
        "                    headers={'Accept': 'application/json'})\n",
        "    response_json = response.json()\n",
        "   \n",
        "    fullURIs=[fullProperty]\n",
        "    #print(response_json)\n",
        "    for hit in response_json:\n",
        "      if(hit[\"predicate\"]!='<http://www.w3.org/2002/07/owl#sameAs>' and hit[\"predicate\"]!='<http://www.w3.org/2002/07/owl#equivalentClass>'):\n",
        "          if(hit[\"predicate\"]=='<http://www.w3.org/2002/07/owl#equivalentProperty>'):\n",
        "              pred1 = hit[\"subject\"].replace(\"<\",\"\").replace(\">\",\"\")\n",
        "              pred2 = hit[\"object\"].replace(\"<\",\"\").replace(\">\",\"\")\n",
        "              pred1Split=pred1.split(\"/\")\n",
        "              pred2Split=pred1.split(\"/\")\n",
        "              if(not pred1 in fullURIs):\n",
        "                sentences.append(re.sub( '(?<!^)(?=[A-Z])', ' ',pred1Split[len(pred1Split)-1]).lower())\n",
        "                fullURIs.append(pred1)\n",
        "              if(not pred2 in fullURIs):\n",
        "                sentences.append(re.sub( '(?<!^)(?=[A-Z])', ' ',pred2Split[len(pred2Split)-1]).lower())\n",
        "                fullURIs.append(pred2)\n",
        "          else:\n",
        "            pred1 = hit[\"predicate\"].replace(\"<\",\"\").replace(\">\",\"\")\n",
        "            pred1Split=pred1.split(\"/\")\n",
        "            if(not pred1 in fullURIs):\n",
        "              sentences.append(re.sub( '(?<!^)(?=[A-Z])', ' ',pred1Split[len(pred1Split)-1]).lower())\n",
        "              fullURIs.append(pred1)\n",
        "\n",
        "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "    embeddings = model.encode(sentences)\n",
        "    pairwise_similarities=np.dot(embeddings,embeddings.T)\n",
        "    retValue=most_similar(sentences,fullURIs,pairwise_similarities,'Cosine Similarity',5)\n",
        "    return retValue\n",
        "\n",
        "currentEntity=\"\"\n",
        "sentences=[\"\"]\n",
        "fullURIs=[\"\"]\n",
        "def getBestPredicateObject(entity,property,fullProperty,object,fullObject,topK):\n",
        "    global currentEntity\n",
        "    global sentences\n",
        "    global fullURIs\n",
        "    sentences[0] = re.sub( '(?<!^)(?=[A-Z])', ' ',property).lower()+ \" \"+re.sub( '(?<!^)(?=[A-Z])', ' ',object).lower()\n",
        "    fullURIs[0]=fullProperty+\" \"+fullObject\n",
        "    if(currentEntity!=entity or len(sentences)==1):\n",
        "      url = \"https://demos.isl.ics.forth.gr/lodsyndesis/rest-api/allFacts?uri=\"+entity\n",
        "      try:\n",
        "        response =requests.get(url, \n",
        "                      headers={'Accept': 'application/json'})\n",
        "        response.raise_for_status()\n",
        "      except requests.exceptions.HTTPError:\n",
        "         response_json = []\n",
        "      else:\n",
        "         response_json = response.json()  \n",
        "\n",
        "     # response_json = response.json()  \n",
        "      currentEntity=entity\n",
        "      #print(response_json)\n",
        "      for hit in response_json:\n",
        "        if(hit[\"predicate\"]!='<http://www.w3.org/2002/07/owl#sameAs>' and hit[\"predicate\"]!='<http://www.w3.org/2002/07/owl#equivalentClass>'):\n",
        "            if(hit[\"predicate\"]!='<http://www.w3.org/2002/07/owl#equivalentProperty>'):\n",
        "              pred1 = hit[\"predicate\"].replace(\"<\",\"\").replace(\">\",\"\")\n",
        "              obj1=hit[\"object\"].replace(\"<\",\"\").replace(\">\",\"\").replace(\"_\",\" \")\n",
        "              pred1Split=pred1.split(\"/\") \n",
        "              if(hit[\"object\"].replace(\"<\",\"\").replace(\">\",\"\")==entity):\n",
        "                obj1=hit[\"subject\"].replace(\"<\",\"\").replace(\">\",\"\").replace(\"_\",\" \")\n",
        "              obj1Split=obj1.split(\"/\")\n",
        "              pred1SplitCell=pred1Split[len(pred1Split)-1].split('#') # to add\n",
        "              if(\"http://www.wikidata.org/entity/\" in pred1):\n",
        "                wkdPred=pred1.replace(\"http://www.wikidata.org/entity/\",\"\").replace(\"c\",\"\").replace(\"*\",\"\")\n",
        "                wkdPredicate=wkdPred\n",
        "                if(wkdPred in wkdProps):\n",
        "                  wkdPredicate=str(wkdProps[wkdPred])\n",
        "                  #print(wkdPredicate)\n",
        "                sentences.append(wkdPredicate+\" \"+re.sub( '(?<!^)(?=[A-Z])', ' ',obj1Split[len(obj1Split)-1]).lower())\n",
        "                fullURIs.append(pred1+ \" \"+hit[\"object\"].replace(\"<\",\"\").replace(\">\",\"\")+ \" \"+hit[\"provenance\"])\n",
        "              else:\n",
        "                sentences.append(re.sub( '(?<!^)(?=[A-Z])', ' ',pred1SplitCell[len(pred1SplitCell)-1]).lower()+\" \"+re.sub( '(?<!^)(?=[A-Z])', ' ',obj1Split[len(obj1Split)-1]).lower())\n",
        "                fullURIs.append(pred1+ \" \"+hit[\"object\"].replace(\"<\",\"\").replace(\">\",\"\")+ \" \"+hit[\"provenance\"])\n",
        "      response =getAllDBpediaTriples(\"<\"+entity+\">\")\n",
        "      currentEntity=entity\n",
        "      #print(response_json)\n",
        "      for hit in response[\"results\"][\"bindings\"]:\n",
        "        if(hit[\"predicate\"][\"value\"]!='http://www.w3.org/2002/07/owl#sameAs' and hit[\"predicate\"][\"value\"]!='http://www.w3.org/2002/07/owl#equivalentClass'):\n",
        "            if(hit[\"predicate\"][\"value\"]!='<http://www.w3.org/2002/07/owl#equivalentProperty>'):\n",
        "              pred1 = hit[\"predicate\"][\"value\"].replace(\"<\",\"\").replace(\">\",\"\")\n",
        "              if(hit[\"object\"][\"type\"]==\"literal\" and \"xml:lang\" in hit[\"object\"].keys() and hit[\"object\"][\"xml:lang\"]!=\"en\"):\n",
        "               continue\n",
        "              obj1=hit[\"object\"][\"value\"].replace(\"<\",\"\").replace(\">\",\"\").replace(\"_\",\" \")\n",
        "              pred1Split=pred1.split(\"/\") \n",
        "              obj1Split=obj1.split(\"/\")\n",
        "              pred1SplitCell=pred1Split[len(pred1Split)-1].split('#') # to add\n",
        "              sentences.append(re.sub( '(?<!^)(?=[A-Z])', ' ',pred1SplitCell[len(pred1SplitCell)-1]).lower()+\" \"+re.sub( '(?<!^)(?=[A-Z])', ' ',obj1Split[len(obj1Split)-1]).lower())\n",
        "              fullURIs.append(pred1+ \" \"+hit[\"object\"][\"value\"].replace(\"<\",\"\").replace(\">\",\"\")+ \" <http://dbpedia.org/current>\")\n",
        "    \n",
        "    else:\n",
        "       print(currentEntity)\n",
        "    \n",
        "\n",
        "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "    #print(sentences)\n",
        "    embeddings = model.encode(sentences)\n",
        "    pairwise_similarities=np.dot(embeddings,embeddings.T)\n",
        "    retValue=most_similar(sentences,fullURIs,pairwise_similarities,'Cosine Similarity',topK)\n",
        "    return retValue\n",
        "\n",
        "# Print the response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Functions for finding the most similar triples by using DBpedia Only**"
      ],
      "metadata": {
        "id": "orcpV4IJuXab"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VKiEmtMFQ8fQ"
      },
      "outputs": [],
      "source": [
        "import json \n",
        "\n",
        "def checkDBpedia(entity,predicate,obj):\n",
        "  # Specify the DBPedia endpoint\n",
        "  sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "  # Query for the description of \"Capsaicin\", filtered by language\n",
        "  sparql.setQuery(\"ASK WHERE { {\"+entity+\" \"+predicate+\" \"+obj+\"}}\")\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  result = sparql.query().convert()\n",
        "  results=\"\"\n",
        "  dictionary=[]\n",
        "  i=0\n",
        "  # The return data contains \"bindings\" (a list of dictionaries)\n",
        "  if(result[\"boolean\"]==True):\n",
        "      # We want the \"value\" attribute of the \"comment\" field\n",
        "      dictionary.append({ \n",
        "      \"predicate\": predicate, \n",
        "      \"provenance\": '<http://dbpedia.org/current>', \n",
        "      \"subject\": entity,\n",
        "      \"object\": obj,\n",
        "      \"threshold\": \"1.0\"\n",
        "     })\n",
        "  if(dictionary==[]):\n",
        "    sparql.setQuery(\"SELECT  ?predicate WHERE { {\"+entity+\" ?predicate \"+ obj+\"} UNION {\"+obj+ \"?predicate \" +entity+\" } . filter(!regex(?predicate,'wiki'))}\")\n",
        "    if('\"' in obj ):\n",
        "       sparql.setQuery(\"SELECT  ?predicate WHERE { \"+entity+\" ?predicate \"+ obj+\" . filter(!regex(?predicate,'wiki'))}\")\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    result = sparql.query().convert()\n",
        "   # print(result)\n",
        "    for hit in result[\"results\"][\"bindings\"]:\n",
        "      # We want the \"value\" attribute of the \"comment\" field\n",
        "       dictionary.append({ \n",
        "      \"predicate\": '<'+hit[\"predicate\"][\"value\"]+'>', \n",
        "      \"provenance\": '<http://dbpedia.org/current>', \n",
        "      \"subject\": entity,\n",
        "      \"object\": obj,\n",
        "      \"threshold\": \"0.5\"\n",
        "     })\n",
        "  if(dictionary==[]):\n",
        "    sparql.setQuery(\"SELECT  ?obj WHERE { {\"+entity+\" \"+predicate+\" ?obj} UNION {?obj \"+predicate+ \" \" +entity+\" }}\")\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    result = sparql.query().convert()\n",
        "  #  print(result)\n",
        "    for hit in result[\"results\"][\"bindings\"]:\n",
        "      objNew=hit[\"obj\"][\"value\"]\n",
        "      if \"http\" in objNew:\n",
        "          objNew=\"<\"+objNew+\">\"\n",
        "      dictionary.append({ \n",
        "      \"predicate\": predicate, \n",
        "      \"provenance\": '<http://dbpedia.org/current>', \n",
        "      \"subject\": entity,\n",
        "      \"object\": objNew,\n",
        "      \"threshold\": \"0.5\"\n",
        "     })\n",
        "  #if(dictionary==[]):\n",
        "    sparql.setQuery(\"SELECT  ?obj WHERE { {\"+entity+\" \"+predicate.replace(\"http://dbpedia.org/ontology/\",\"http://dbpedia.org/property/\")+\" ?obj} UNION {?obj \"+predicate.replace(\"http://dbpedia.org/ontology/\",\"http://dbpedia.org/property/\")+ \" \" +entity+\" }}\")\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    result = sparql.query().convert()\n",
        "   # print(result)\n",
        "    for hit in result[\"results\"][\"bindings\"]:\n",
        "      objNew=hit[\"obj\"][\"value\"]\n",
        "      if \"http\" in objNew:\n",
        "        objNew=\"<\"+objNew+\">\"\n",
        "      # We want the \"value\" attribute of the \"comment\" field\n",
        "      dictionary.append({ \n",
        "      \"predicate\": predicate.replace(\"http://dbpedia.org/ontology/\",\"http://dbpedia.org/property/\"), \n",
        "      \"provenance\": '<http://dbpedia.org/current>', \n",
        "      \"subject\": entity,\n",
        "      \"object\": objNew,\n",
        "      \"threshold\": \"0.5\"\n",
        "     })\n",
        "  return dictionary\n",
        "#checkDBpedia(\"<http://dbpedia.org/resource/Aristotle>\",\"<http://dbpedia.org/ontology/birthPlace>\",\"<http://dbpedia.org/resource/Stagira_(ancient_city)>\")\n",
        "\n",
        "def getAllDBpediaTriples(entity):\n",
        "  sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "  # Query for the description of \"Capsaicin\", filtered by language\n",
        "  sparql.setQuery(\"SELECT * WHERE { {\"+entity+\" ?predicate ?object } UNION {?object ?predicate \" +entity+\" }. filter(!regex(?predicate,'wiki'))}\")\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  result = sparql.query().convert()\n",
        "  return result\n",
        "#print(getAllDBpediaTriples(\"<http://dbpedia.org/resource/Nikos_Zisis>\"))\n",
        "\n",
        "def getBestPredicateObjectDBpedia(entity,property,fullProperty,object,fullObject,topK):\n",
        "    global currentEntity\n",
        "    global sentences\n",
        "    global fullURIs\n",
        "    sentences[0] = re.sub( '(?<!^)(?=[A-Z])', ' ',property).lower()+ \" \"+re.sub( '(?<!^)(?=[A-Z])', ' ',object).lower()\n",
        "    fullURIs[0]=fullProperty+\" \"+fullObject\n",
        "    if(currentEntity!=entity or len(sentences)==1):\n",
        "      response =getAllDBpediaTriples(entity)\n",
        "      currentEntity=entity\n",
        "      #print(response_json)\n",
        "      for hit in response[\"results\"][\"bindings\"]:\n",
        "        if(hit[\"predicate\"][\"value\"]!='http://www.w3.org/2002/07/owl#sameAs' and hit[\"predicate\"][\"value\"]!='http://www.w3.org/2002/07/owl#equivalentClass'):\n",
        "            if(hit[\"predicate\"][\"value\"]!='<http://www.w3.org/2002/07/owl#equivalentProperty>'):\n",
        "              pred1 = hit[\"predicate\"][\"value\"].replace(\"<\",\"\").replace(\">\",\"\")\n",
        "              if(hit[\"object\"][\"type\"]==\"literal\" and \"xml:lang\" in hit[\"object\"].keys() and hit[\"object\"][\"xml:lang\"]!=\"en\"):\n",
        "               continue\n",
        "              obj1=hit[\"object\"][\"value\"].replace(\"<\",\"\").replace(\">\",\"\").replace(\"_\",\" \")\n",
        "              pred1Split=pred1.split(\"/\") \n",
        "              obj1Split=obj1.split(\"/\")\n",
        "              pred1SplitCell=pred1Split[len(pred1Split)-1].split('#') # to add\n",
        "              sentences.append(re.sub( '(?<!^)(?=[A-Z])', ' ',pred1SplitCell[len(pred1SplitCell)-1]).lower()+\" \"+re.sub( '(?<!^)(?=[A-Z])', ' ',obj1Split[len(obj1Split)-1]).lower())\n",
        "              fullURIs.append(pred1+ \" \"+hit[\"object\"][\"value\"].replace(\"<\",\"\").replace(\">\",\"\"))\n",
        "    else:\n",
        "       print(currentEntity)\n",
        "    \n",
        "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "    #print(sentences)\n",
        "    embeddings = model.encode(sentences)\n",
        "    pairwise_similarities=np.dot(embeddings,embeddings.T)\n",
        "    retValue=most_similar(sentences,fullURIs,pairwise_similarities,'Cosine Similarity',topK)\n",
        "    return retValue"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Calculate Similarity for the Candidates**"
      ],
      "metadata": {
        "id": "8-EKDrIauaji"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "385SqLNWnraP"
      },
      "outputs": [],
      "source": [
        "def calculateSimilarity(pred1, obj1,pred2, obj2):\n",
        "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "    newPred1=pred1.replace(\">\",\"\").replace('_',' ').split(\"/\")\n",
        "    newObj1=obj1.replace(\">\",\"\").replace('\"','').replace('_','').split(\"/\")\n",
        "    sentence1= re.sub( '(?<!^)(?=[A-Z])', ' ',newPred1[len(newPred1)-1]).lower()+ \" \"+re.sub( '(?<!^)(?=[A-Z])', ' ',newObj1[len(newObj1)-1]).lower()\n",
        "    #print(pred2)\n",
        "    if(\"http://www.wikidata.org/entity/\" in pred2):\n",
        "        wkdPred=pred2.replace(\"http://www.wikidata.org/entity/\",\"\").replace(\"c\",\"\").replace(\"*\",\"\").replace(\">\",\"\").replace(\"<\",\"\")\n",
        "        wkdPredicate=wkdPred\n",
        "        if(wkdPred in wkdProps):\n",
        "          wkdPredicate=str(wkdProps[wkdPred])\n",
        "          #print(wkdPredicate)\n",
        "        newObj2=obj2.replace(\">\",\"\").replace('_',' ').replace('\"','').split(\"/\")\n",
        "        sentence2= wkdPredicate+\" \"+re.sub( '(?<!^)(?=[A-Z])', ' ',newObj2[len(newObj2)-1]).lower()\n",
        "    else:\n",
        "      newPred2=pred2.replace(\">\",\"\").replace('_',' ').split(\"/\")\n",
        "      newObj2=obj2.replace(\">\",\"\").replace('_',' ').replace('\"','').split(\"/\")\n",
        "      sentence2= re.sub( '(?<!^)(?=[A-Z])', ' ',newPred2[len(newPred2)-1]).lower()+ \" \"+re.sub( '(?<!^)(?=[A-Z])', ' ',newObj2[len(newObj2)-1]).lower()\n",
        "    #print(sentence1, sentence2)\n",
        "    sentencesBoth=[sentence1,sentence2]\n",
        "    embeddings = model.encode(sentencesBoth)\n",
        "    pairwise_similarities=np.dot(embeddings,embeddings.T)\n",
        "    return str(pairwise_similarities[0][1])\n",
        "\n",
        "def sortSimilarities(sentences,topK):\n",
        "    sentencesSplit=sentences.split(\"\\n\")\n",
        "    key_value = {}\n",
        "    cnt=0\n",
        "    for entry in sentencesSplit:\n",
        "      entrySplit=entry.split(\"\\t\")\n",
        "      if len(entrySplit)==2:\n",
        "        threshold=entrySplit[0]\n",
        "        triple=entrySplit[1]\n",
        "        key_value[threshold]=triple\n",
        "        #print(threshold)\n",
        "        #print(triple)\n",
        "    for i in sorted(key_value.keys(),reverse=True):\n",
        "        print(i,key_value[i] )\n",
        "        cnt=cnt+1\n",
        "        if(cnt==topK):\n",
        "          break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. The process of validating the facts. You need to configure the path containing the CHATGPT triples, if DBpedia will be used alone or not and the number of K (for the most similar)**"
      ],
      "metadata": {
        "id": "OZtFWhIxujT7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfXsgAXYMg2R"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "factsGPT=\"greekPersons.nt\"\n",
        "dbpediaOnly=False #onlyDBpediaValue\n",
        "topK=3 #topKValue\n",
        "#triplesChecker(factsGPT,True,3)\n",
        "\n",
        "#def triplesChecker(factsGPT,onlyDBpediaValue,topKValue):\n",
        "tripleID=1\n",
        "correctCount=0\n",
        "samePredicateOrObjectCount=0\n",
        "bestMatchCount=0\n",
        "retValue=\"\"\n",
        "currentEntity=\"\"\n",
        "response_json=[]\n",
        "sentences=[\"\"]\n",
        "fullURIs=[\"\"]\n",
        "\n",
        "factsSplit = open(factsGPT, \"r\")\n",
        "#factsSplit=factsGPT.split(\"\\n\")\n",
        "for fct in factsSplit:\n",
        "  start = time.time()\n",
        "  factSplit=fct.split(\"> \")\n",
        "  dbpediaTriples=[]\n",
        "  if(len(factSplit)>=3):\n",
        "    entity=factSplit[0].split(\"<\")[1]\n",
        "    predicateSplit=factSplit[1].replace(\"<\",\"\").split(\"#\")\n",
        "    predicate=predicateSplit[len(predicateSplit)-1]\n",
        "    obj=factSplit[2].replace(\"<\",\"\").replace('\"',\"\").replace(\" .\",\"\").split(\"^^\")\n",
        "    if(obj[0].isnumeric()):\n",
        "      obj[0]='\"'+obj[0]+'\"'\n",
        "    facts=predicate+ \" \"+obj[0]\n",
        "    \n",
        "    if \"http\" in factSplit[2].split(\"^^\")[0]:\n",
        "      dbpediaTriples=checkDBpedia(\"<\"+entity+\">\",factSplit[1]+\">\",factSplit[2].replace(\" .\",\"\").split(\"^^\")[0]+\">\")\n",
        "    else:\n",
        "      dbpediaTriples=checkDBpedia(\"<\"+entity+\">\",factSplit[1]+\">\",factSplit[2].replace(\" .\",\"\").split(\"^^\")[0])\n",
        "    if(dbpediaOnly==False):\n",
        "      url = \"https://demos.isl.ics.forth.gr/lodsyndesis/rest-api/factChecking?uri=\"+entity+\"&fact=\"+facts+\"&threshold=0.5\"\n",
        "      # A GET request to the API\n",
        "      response =requests.get(url, \n",
        "                      headers={'Accept': 'application/json'})\n",
        "      \n",
        "  else:\n",
        "    continue\n",
        "# Print the response\n",
        "  if dbpediaOnly==False:\n",
        "      response_json = response.json()\n",
        "  else:\n",
        "      response_json = []\n",
        "  if(response_json==[] and \"http://dbpedia.org/ontology/\" in facts and dbpediaOnly==False):\n",
        "    url = \"https://demos.isl.ics.forth.gr/lodsyndesis/rest-api/factChecking?uri=\"+entity+\"&fact=\"+facts.replace(\"http://dbpedia.org/ontology/\",\"http://dbpedia.org/property/\")+\"&threshold=0.5\"\n",
        "    #print(url)\n",
        "    # A GET request to the API\n",
        "    response =requests.get(url, \n",
        "                    headers={'Accept': 'application/json'})\n",
        "    response_json = response.json()\n",
        "\n",
        "  correct=\"\"\n",
        "  samePredicateOrObject=\"\"\n",
        "  bestMatch=\"\"\n",
        "  if(entity!=currentEntity):\n",
        "    sentences=[\"\"]\n",
        "    fullURIs=[\"\"]\n",
        "\n",
        "  #print(dbpediaTriples)\n",
        "  response_json.extend(dbpediaTriples)\n",
        "  #print(response_json)\n",
        "  # print(response_json)\n",
        "  if(response_json==[]):\n",
        "    newPred=predicate.split(\"/\")\n",
        "    newObj=obj[0].split(\"/\")\n",
        "    #print(entity,newPred[len(newPred)-1],predicate,newObj[len(newObj)-1].replace(\"_\",\" \"),obj[0])\n",
        "    if(dbpediaOnly==False):\n",
        "      bestPredicate=  getBestPredicateObject(entity,newPred[len(newPred)-1],predicate,newObj[len(newObj)-1].replace(\"_\",\" \"),obj[0],topK) # getBestPredicateObjectDBpedia(\"<\"+entity+\">\",newPred[len(newPred)-1],predicate,newObj[len(newObj)-1].replace(\"_\",\" \"),obj[0],topK) # #getBestPredicate(entity,newPred[len(newPred)-1],predicate) \n",
        "      retValue=str(bestPredicate)\n",
        "    else:\n",
        "      bestPredicate= getBestPredicateObjectDBpedia(\"<\"+entity+\">\",newPred[len(newPred)-1],predicate,newObj[len(newObj)-1].replace(\"_\",\" \"),obj[0],topK)  # getBestPredicateObjectDBpedia(\"<\"+entity+\">\",newPred[len(newPred)-1],predicate,newObj[len(newObj)-1].replace(\"_\",\" \"),obj[0],topK) # #getBestPredicate(entity,newPred[len(newPred)-1],predicate) \n",
        "      retValue=str(bestPredicate)\n",
        "  else:\n",
        "    for entry in response_json:\n",
        "      if entry[\"threshold\"]==\"1.0\":\n",
        "        correct=entry[\"threshold\"]+\"\\t\"+entry[\"subject\"]+ \" \"+entry[\"predicate\"]+\" \"+entry[\"object\"]+\" \"+ entry[\"provenance\"]+\" Same Triple\\n\"\n",
        "      else:\n",
        "        if(\"<\"+predicate.replace(\"property\",\"ontology\")+\">\"==entry[\"predicate\"].replace(\"property\",\"ontology\") and obj[0].replace('\"',\"\").replace(\"<\",\"\").replace(\">\",\"\").lower()==entry[\"object\"].replace('\"',\"\").replace(\"<\",\"\").replace(\">\",\"\").lower()):\n",
        "            correct=\"1.0\\t\"+entry[\"subject\"]+ \" \"+entry[\"predicate\"]+\" \"+entry[\"object\"]+\" \"+ entry[\"provenance\"]+\" Same Triple\\n\"\n",
        "        elif(\"<\"+predicate.replace(\"property\",\"ontology\")+\">\"==entry[\"predicate\"].replace(\"property\",\"ontology\") or entry[\"predicate\"]==\"<http://www.w3.org/2006/vcard/ns#type>\"):\n",
        "            similarity=calculateSimilarity(predicate,obj[0],entry[\"predicate\"],entry[\"object\"])\n",
        "            samePredicateOrObject+=similarity+\"\\t\"+entry[\"subject\"]+ \" \"+entry[\"predicate\"]+\" \"+entry[\"object\"]+\" \"+ entry[\"provenance\"]+\" Same Predicate - Different Object\\n\"\n",
        "        elif(obj[0].replace('\"',\"\").replace(\"<\",\"\").replace(\">\",\"\").lower()==entry[\"object\"].replace('\"',\"\").replace(\"<\",\"\").replace(\">\",\"\").lower()):\n",
        "          similarity=calculateSimilarity(predicate,obj[0],entry[\"predicate\"],entry[\"object\"])\n",
        "          samePredicateOrObject+=similarity+\"\\t\"+entry[\"subject\"]+ \" \"+entry[\"predicate\"]+\" \"+entry[\"object\"]+\" \"+ entry[\"provenance\"]+\" Same Object - Different Predicate\\n\"\n",
        "        else:\n",
        "          similarity=calculateSimilarity(predicate,obj[0],entry[\"predicate\"],entry[\"object\"])\n",
        "          bestMatch+=similarity+\"\\t\"+entry[\"subject\"]+ \" \"+entry[\"predicate\"]+\" \"+entry[\"object\"]+\" \"+ entry[\"provenance\"]+\" Most Similar Triples\\n\"\n",
        "  print(\"#\"+str(tripleID)+\" \"+\" ChatGPT Triple: \"+fct)\n",
        "  tripleID=tripleID+1\n",
        "  print(\"Fact Checking Triple(s) and Provenance\")\n",
        "  if(correct!=\"\"):\n",
        "    print(correct)\n",
        "    correctCount=correctCount+1\n",
        "  #elif(sameObject!=\"\"):\n",
        "  # print(sameObject)\n",
        "  #  sortSimilarities(sameObject,topK)\n",
        "  # sameObjectCount=sameObjectCount+1\n",
        "  elif(samePredicateOrObject!=\"\"):\n",
        "    #print(samePredicate)\n",
        "    sortSimilarities(samePredicateOrObject,topK)\n",
        "    samePredicateOrObjectCount=samePredicateOrObjectCount+1\n",
        "  elif(bestMatch!=\"\"):\n",
        "    sortSimilarities(bestMatch,topK)\n",
        "    #print(bestMatch)\n",
        "    bestMatchCount=bestMatchCount+1\n",
        "  else:\n",
        "    print(retValue)\n",
        "    bestMatchCount=bestMatchCount+1\n",
        "\n",
        "\n",
        "  end = time.time()\n",
        "  print(\"ElapsedTime: \",end - start)\n",
        "  print(\"\\n\")\n",
        "print(\"A. Same or Equivalent:\" +str(correctCount))\n",
        "print(\"B. Same Predicate or Object:\" +str(samePredicateOrObjectCount))\n",
        "print(\"C. Most SImilar Triple(s):\" +str(bestMatchCount))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Appendix Code. Additional Code for checking availability of URIs/properties in DBpedia**"
      ],
      "metadata": {
        "id": "xzpl0vBexGDi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iNO8jgR80tf"
      },
      "outputs": [],
      "source": [
        "def checkURIExistence(URIs,isProperty):\n",
        "  for entity in URIs:\n",
        "      sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "      # Query for the description of \"Capsaicin\", filtered by language\n",
        "      sparql.setQuery(\"ASK WHERE { {\"+entity+\" ?p ?o} UNION {?o ?p \"+entity+\"}}\")\n",
        "      if(isProperty):\n",
        "         sparql.setQuery(\"ASK WHERE { {?s \"+entity+\"  ?o}}\")\n",
        "      sparql.setReturnFormat(JSON)\n",
        "      result = sparql.query().convert()\n",
        "      #print(result)\n",
        "      results=\"\"\n",
        "      dictionary=[]\n",
        "      i=0\n",
        "      # The return data contains \"bindings\" (a list of dictionaries)\n",
        "      if(result[\"boolean\"]==True):\n",
        "        print(entity+\" exists\")\n",
        "      else:\n",
        "         print(entity+\" notAvailable\")\n",
        "URIs=[]\n",
        "Properties=[]\n",
        "\n",
        "\n",
        "factsGPT=\"greekPersons.nt\"\n",
        "factsSplit = open(factsGPT, \"r\")\n",
        "for x in factsSplit:\n",
        "  factSplit=x.split(\"> \")\n",
        "  dbpediaTriples=[]\n",
        "  if(len(factSplit)>=3):\n",
        "    predicate=factSplit[1]+\">\"\n",
        "    if(predicate in Properties):\n",
        "      i=0\n",
        "    else:\n",
        "      Properties.append(predicate)\n",
        "    subject=factSplit[0]+\">\"\n",
        "    if(subject in URIs):\n",
        "      i=0\n",
        "    else:\n",
        "      URIs.append(subject)\n",
        "    obj=factSplit[2].replace(\" .\",\"\").split(\"^^\")[0]+\">\"\n",
        "    if \"<\" in obj:\n",
        "       if(obj in URIs):\n",
        "         i=0\n",
        "       else:\n",
        "         URIs.append(obj)\n",
        "   \n",
        "print(\"Properties\\n\")\n",
        "checkURIExistence(Properties,True)\n",
        "print(\"\\n\\nResources\\n\")\n",
        "checkURIExistence(URIs,False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Appendix Code for implementing the pipelines**"
      ],
      "metadata": {
        "id": "XJgIavlGxUie"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-5JzfuQIasx"
      },
      "outputs": [],
      "source": [
        "#!pip install openai\n",
        "import openai\n",
        "\n",
        "# Define OpenAI API key \n",
        "openai.api_key = \"x\"\n",
        "\n",
        "# Set up the model and prompt\n",
        "model_engine = \"text-davinci-003\"\n",
        "#prompt = \"find me all the entities and their DBpedia links from the following text: The Godfather is an American crime film directed by Francis Ford Coppola and produced by Albert S. Ruddy, based on Mario Puzo's best-selling novel of the same name. The film features an ensemble cast including Marlon Brando, Al Pacino, James Caan, Richard Castellano, Robert Duvall, Sterling Hayden, John Marley, Richard Conte, and Diane Keaton. The story, spanning from 1945 to 1955, chronicles the Corleone crime family under patriarch Vito Corleone (Brando), focusing on the transformation of one of his sons, Michael Corleone (Pacino), from reluctant family outsider to ruthless mafia boss.\"\n",
        "prompt=\"give me 5 RDF N-triples using DBpedia format for El Greco\"\n",
        "# Generate a response\n",
        "completion = openai.Completion.create(\n",
        "    engine=model_engine,\n",
        "    prompt=prompt,\n",
        "    max_tokens=1024,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    temperature=0.5,\n",
        ")\n",
        "\n",
        "factsGPT=completion.choices[0].text\n",
        "print(factsGPT)\n",
        "triplesChecker(factsGPT,True,3)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}